# ModuleFormer
We propose a new neural network architecture, ModuleFormer, that leverages modularity to improve the efficiency and flexibility of large language models.
